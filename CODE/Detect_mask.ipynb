{"cells":[{"cell_type":"markdown","metadata":{"id":"xiq9pJbkpcpu"},"source":["# 1. 데이터 수집하기\n","---\n","모델에 학습을 시키기 위해서는 마스크를 쓴 사진과 쓰지 않은 사진 그리고 착용 유무에 따른 레이블 데이터 셋트가 필요하다. 아직은 이와 관련된 데이터 셋트가 공개되지 않은 것으로 판단되어서 직접 데이터 셋트를 만들기로 하였다.\n","\n","## 1.1 얼굴 데이터 셋트 만들기 \n","\n","직접 얼굴 데이터 셋트를 만들기 위해서 cvlib를 사용했다. cvlib는 파이썬에서 얼굴 인식을 사용하기 위해 사용되는 라이브러리이다. cvlib 라이브러리를 사용하기 위해서 `opencv-python`과 `tensorflow`, `cvlib` 라이브러리가 설치되어야 한다. `pip install opencv-ptyon tensorflow cvlib` 명령으로 이 라이브러리들을 한꺼번에 설치할 수 있다. \n","\n","### 1.1.1 라이브러리 임포트 하기\n","우선 이 프로젝트에서 사용할 라이브러리를 모두 임포트한다. "]},{"cell_type":"code","source":["pip install opencv-python tensorflow"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9LapVw4KpVWm","executionInfo":{"status":"ok","timestamp":1639715276104,"user_tz":-540,"elapsed":7942,"user":{"displayName":"OH Hakdong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihTg2hhSkeSH2CzD-I0kzuWEn918kbaPomc63ZzA=s64","userId":"18209854005101912183"}},"outputId":"a64981c3-1477-4723-fc7c-b504fa7be13b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.1.2.30)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.7.0)\n","Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python) (1.19.5)\n","Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (12.0.0)\n","Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.13.3)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n","Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n","Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n","Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n","Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n","Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.10.0.2)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.42.0)\n","Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.37.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.22.0)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (57.4.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (3.3.6)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (2.23.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.8.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.6.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.24.3)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n"]}]},{"cell_type":"code","source":["pip install cvlib"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FHvVlAiRrSm2","executionInfo":{"status":"ok","timestamp":1639715727719,"user_tz":-540,"elapsed":3072,"user":{"displayName":"OH Hakdong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihTg2hhSkeSH2CzD-I0kzuWEn918kbaPomc63ZzA=s64","userId":"18209854005101912183"}},"outputId":"b8c19e60-e51f-4bda-898d-9201643c817b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: cvlib in /usr/local/lib/python3.7/dist-packages (0.2.6)\n","Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from cvlib) (2.4.1)\n","Requirement already satisfied: imutils in /usr/local/lib/python3.7/dist-packages (from cvlib) (0.5.4)\n","Requirement already satisfied: progressbar in /usr/local/lib/python3.7/dist-packages (from cvlib) (2.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from cvlib) (2.23.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from cvlib) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from cvlib) (1.19.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->cvlib) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->cvlib) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->cvlib) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->cvlib) (1.24.3)\n"]}]},{"cell_type":"code","source":["import cv2\n","\n","cap = cv2.VideoCapture(0)               # 0번 카메라 장치 연결 ---①\n","if cap.isOpened():                      # 캡쳐 객체 연결 확인\n","    while True:\n","        ret, img = cap.read()           # 다음 프레임 읽기\n","        if ret:\n","            cv2.imshow('camera', img)   # 다음 프레임 이미지 표시\n","            if cv2.waitKey(1) != -1:    # 1ms 동안 키 입력 대기 ---②\n","                break                   # 아무 키라도 입력이 있으면 중지\n","        else:\n","            print('no frame')\n","            break\n","else:\n","    print(\"can't open camera.\")\n","cap.release()                           # 자원 반납\n","cv2.destroyAllWindows()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w6vdnjiIusLJ","executionInfo":{"status":"ok","timestamp":1639717486297,"user_tz":-540,"elapsed":6,"user":{"displayName":"OH Hakdong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihTg2hhSkeSH2CzD-I0kzuWEn918kbaPomc63ZzA=s64","userId":"18209854005101912183"}},"outputId":"1dde44cd-bad0-4541-c87b-a07fec304655"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["can't open camera.\n"]}]},{"cell_type":"code","execution_count":26,"metadata":{"id":"q3mWektjpcp1","executionInfo":{"status":"ok","timestamp":1639717489039,"user_tz":-540,"elapsed":5,"user":{"displayName":"OH Hakdong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihTg2hhSkeSH2CzD-I0kzuWEn918kbaPomc63ZzA=s64","userId":"18209854005101912183"}}},"outputs":[],"source":["import cv2\n","import cvlib\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import pandas as pd\n","import seaborn as sns\n","from IPython.display import Image\n","from sklearn.metrics import confusion_matrix\n","from tensorflow.keras.preprocessing.image import array_to_img, load_img, img_to_array\n","from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.models import load_model"]},{"cell_type":"markdown","metadata":{"id":"OonPr_BXpcp4"},"source":["### 1.1.2 웹캠 연결하기\n","얼굴 데이터를 캡쳐하기 위해서 웹캠을 사용한다. cv2.VideoCatpure(0)를 사용해서 컴퓨터에 연결된 웹캠을 연결한다. 그리고 웹캠이 제대로 연결되었으면 isOpened()는 True가 리턴된다. "]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":201},"id":"DC_RzYQKpcp5","executionInfo":{"status":"error","timestamp":1639717492123,"user_tz":-540,"elapsed":6,"user":{"displayName":"OH Hakdong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihTg2hhSkeSH2CzD-I0kzuWEn918kbaPomc63ZzA=s64","userId":"18209854005101912183"}},"outputId":"585e1e8c-9a62-44a6-e774-5da480a1afa2"},"outputs":[{"output_type":"error","ename":"Exception","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-dc731452e46d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwebcam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwebcam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misOpened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"웹캡이 연결되지 않았습니다.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mException\u001b[0m: 웹캡이 연결되지 않았습니다."]}],"source":["webcam = cv2.VideoCapture(0)\n","if not webcam.isOpened():\n","    raise Exception(\"웹캡이 연결되지 않았습니다.\")"]},{"cell_type":"markdown","metadata":{"id":"vR_we7hipcp5"},"source":["### 1.1.2 웹캠으로 사진 캡쳐하기\n","webcam.read()를 사용하여 웹캠으로 사진을 캡쳐할 수 있다. read() 함수는 두개의 값을 튜플로 리턴한다. 첫 번째 값은 캡쳐가 되었으면 True, 안되었으면 False가 되고, 두 번째 값은 캡쳐된 이미지를 리턴한다. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ICWfgN3Npcp6","executionInfo":{"status":"error","timestamp":1639715689073,"user_tz":-540,"elapsed":338,"user":{"displayName":"OH Hakdong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihTg2hhSkeSH2CzD-I0kzuWEn918kbaPomc63ZzA=s64","userId":"18209854005101912183"}},"colab":{"base_uri":"https://localhost:8080/","height":201},"outputId":"b6da5c71-5480-4679-f8b8-8df0f380ae8c"},"outputs":[{"output_type":"error","ename":"Exception","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-a0b89fdfcc89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mis_readed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwebcam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_readed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"캡쳐가 제대로 되지 않았습니다.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mException\u001b[0m: 캡쳐가 제대로 되지 않았습니다."]}],"source":["is_readed, frame = webcam.read()\n","if not is_readed:\n","    raise Exception(\"캡쳐가 제대로 되지 않았습니다.\")"]},{"cell_type":"markdown","metadata":{"id":"PKdmkg6lpcp7"},"source":["### 1.1.3 얼굴 탐지하기\n","cvlib.detect_face() 함수를 사용하면 얼굴을 탐지할 수 있다. detect_face() 함수에 이미지를 전달하면 두개의 값을 튜플로 리턴된다. 첫 번째 값은 얼굴의 위치를 나타내고, 두 번째 값은 얼굴일 확률을 나타낸다. "]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"Izld_Ms_pcp8","executionInfo":{"status":"error","timestamp":1639715694230,"user_tz":-540,"elapsed":314,"user":{"displayName":"OH Hakdong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihTg2hhSkeSH2CzD-I0kzuWEn918kbaPomc63ZzA=s64","userId":"18209854005101912183"}},"colab":{"base_uri":"https://localhost:8080/","height":201},"outputId":"35ea51e4-20f6-47d1-cc63-eb73e9cab776"},"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-c9eb04db54df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfidences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcvlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_face\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfaces\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfidences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"]}],"source":["faces, confidences = cvlib.detect_face(frame)\n","print(faces)\n","print(confidences)"]},{"cell_type":"markdown","metadata":{"id":"fAW_OAZXpcp9"},"source":["### 1.1.4 이미지 저장하기\n","cv2.imwrite() 함수를 사용하면 이미지를 저장할 수 있다. 첫 번째 인자는 저장할 파일의 위치와 파일이름을 전달하고, 두 번째 인자는 저장할 이미지를 전달한다. cvlib.detect_face() 함수로 찾은 얼굴 위치를 사용해서 얼굴만 추출하여 저장한다. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I6KDHzSNpcp-"},"outputs":[],"source":["start_x, start_y, end_x, end_y = faces[0]\n","cv2.imwrite('1.jpg', frame[start_y:end_y, start_x:end_x, :])"]},{"cell_type":"markdown","metadata":{"id":"-OOAVSpMpcp_"},"source":["### 1.1.5 웹캠으로 촬영 후 얼굴만 추출하여 저장하는 함수 만들기\n","1.1.1 ~ 1.1.4의 코드를 이용해서 웹캠으로 촬영 후 얼굴만 추출하여 저장하는 함수로 만들었다. 이 함수를 이용해서 마스크를 쓴 얼굴 사진 300장과 마스크를 쓰지 않은 얼굴 사진 300장을 만들었다. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cU6OW8LOpcqA"},"outputs":[],"source":["import time\n","import cv2\n","import cvlib\n","\n","\n","def capture_n_save_face(path, max=1):\n","    count = 0\n","    \n","    # 웹캠 연결하기\n","    webcam = cv2.VideoCapture(0)\n","    if not webcam.isOpened():\n","        raise Exception(\"웹캡이 연결되지 않았습니다.\")\n","        \n","    while count < max:\n","        time.sleep(0.3)   # 다양한 얼굴이 저장될 수 있도록 0.3초 지연을 준다. \n","        is_readed, frame = webcam.read() # 웹캠으로 촬영하기 \n","\n","        if not is_readed:\n","            raise Exception(\"캡쳐가 제대로 되지 않았습니다.\")\n","\n","        faces, confidences = cvlib.detect_face(frame)   # 얼굴 탐지하기\n","\n","        for face, confidence in zip(faces, confidences):\n","            if confidence < 0.8:   # 얼굴일 확률이 80% 이하면 패스한다.\n","                continue\n","            start_x, start_y, end_x, end_y = face\n","            try:\n","                cv2.imwrite(path+str(count)+'.jpg', frame[start_y:end_y, start_x:end_x, :])   # 얼굴만 저장하기 \n","                count += 1\n","            except:\n","                break\n","        \n","        print(count, end=' ')\n","\n","    webcam.release()\n","\n","\n","# capture_n_save_face('c:\\\\non_mask\\\\', 300) # 마스크를 쓰지 않은 사진 300만 만들기\n","# capture_n_save_face('c:\\\\mask\\\\', 300) # 마스크를 쓴 사진 300장 만들기\n"]},{"cell_type":"markdown","metadata":{"id":"B8T5x9bxpcqB"},"source":["### 1.1.6 만들어진 사진 확인하기\n","captur_n_save_face() 함수를 사용하여 다양한 각도로 사진이 캡쳐될 수 있도록 촬영하였다."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"vdsG2_VVpcqB"},"outputs":[],"source":["Image(\"non_mask_images.png\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vviURSyxpcqC"},"outputs":[],"source":["Image(\"mask_images.png\")"]},{"cell_type":"markdown","metadata":{"id":"LyVeIJPPpcqC"},"source":["# 2. 이미지 크기 파악하기\n","---\n","딥러닝 모델에 데이터를 학습하기 위해서 통일된 데이터의 크기로 전처리를 해야 한다. 그래서 전처리 전에 모든 이미지 데이터의 크기를 그래프를 사용하여 확인한 후 적절한 크기를 찾아보겠다. \n","\n","## 2.1 이미지 목록 불러오기\n","\n","이미지를 불러 오기 전에 우선 이미지의 이름을 알아야 한다. os.listdir() 함수를 사용하면 디렉터리 안에 있는 파일 리스트를 얻을 수 있다. 마스크를 쓰지 않는 이미지는 c:\\non_mask\\에 마스크를 쓴 이미지는 c:\\mask\\에 모아두었다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O6PP9TEYpcqD"},"outputs":[],"source":["non_mask_list = os.listdir('c:\\\\non_mask\\\\')\n","mask_list = os.listdir('c:\\\\mask\\\\')"]},{"cell_type":"markdown","metadata":{"id":"VqlUNWZVpcqD"},"source":["## 2.2 이미지 목록을 이용하여 이미지 크기 얻기\n","\n","cv2.imread()로 이미지를 불러온 후 shape 속성에 접근하면 (세로 * 가로 * 채널) 정보를 튜플로 얻을 수 있다. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GTSbw7qhpcqD"},"outputs":[],"source":["image = cv2.imread(\"c:\\\\non_mask\\\\\" + non_mask_list[0])\n","image.shape"]},{"cell_type":"markdown","metadata":{"id":"R7t_ewP_pcqE"},"source":["이제 모든 이미지의 shape 속성을 이용해서 가로 정보를 가지는 widths 리스트와 세로 정보를 가지는 heights 리스트를 만들어 보자."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fBOiDQ-3pcqE"},"outputs":[],"source":["widths = []\n","heights = []\n","\n","for non_mask in non_mask_list:\n","    image = cv2.imread(\"c:\\\\non_mask\\\\\" + non_mask)\n","    widths.append(image.shape[1])\n","    heights.append(image.shape[0])\n","    \n","for mask in mask_list:\n","    image = cv2.imread(\"c:\\\\mask\\\\\" + mask)\n","    widths.append(image.shape[1])\n","    heights.append(image.shape[0])"]},{"cell_type":"markdown","metadata":{"id":"6R-jdm7WpcqE"},"source":["## 2.3 이미지 크기를 그래프로 확인하기\n","\n","이미지 크기 분포를 보기 좋게 하기 위해서 그래프를 이용한다. \n","\n","### 2.3.1 삼전도로 출력하기\n","\n","matploglib 라이브러리를 사용하여 가로, 세로로 삼전도를 출력해보니 대부분 일정 비율로 이미지 크기가 이루어지는 것을 확인 할 수 있다."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"c0DM96vrpcqF"},"outputs":[],"source":["plt.figure(figsize=(10, 5))\n","plt.scatter(widths, heights, alpha=0.3)"]},{"cell_type":"markdown","metadata":{"id":"sh-UeZIzpcqF"},"source":["### 2.3.2 히스토그램으로 출력하기\n","\n","#### 가로 크기 히스토그램\n","\n","가로 크기를 히스토그램으로 나타내어 보자."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"JE7ITvZgpcqF"},"outputs":[],"source":["cut_widths = pd.cut(widths, np.arange(50, 260, 10)).value_counts()\n","\n","fig, ax = plt.subplots(figsize=(26, 7))\n","ax.bar(range(len(cut_widths)),cut_widths.values)\n","ax.set_xticks(range(len(cut_widths)))\n","ax.set_xticklabels(cut_widths.index)\n","ax"]},{"cell_type":"markdown","metadata":{"id":"f_E5HkIVpcqG"},"source":["#### 세로 크기 히스토그램\n","\n","동일한 방법으로 세로 크기를 히스토그램을 나타내어 보자."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"w1-QW7_rpcqG"},"outputs":[],"source":["cut_heights = pd.cut(heights, np.arange(50, 260, 10)).value_counts()\n","\n","fig, ax = plt.subplots(figsize=(25,7))\n","ax.bar(range(len(cut_heights)),cut_heights.values)\n","ax.set_xticks(range(len(cut_heights)))\n","ax.set_xticklabels(cut_heights.index)\n","ax"]},{"cell_type":"markdown","metadata":{"id":"L0L4Vuq7pcqG"},"source":["### 2.3.3 모델에 사용할 이미지 크기 결정하기 \n","모델에 학습을 시키기 위해서는 같은 크기로 만들어야 한다. 너무 작은 이미지를 크게 만들게 되면 정보 손실이 발생하고 너무 큰 이미지를 작게 만들면 정보량이 부족해 진다. 그래서 위의 그래프를 참고해서 적절한 크기를 찾아야 한다.\n","\n","가로는 90\\~190, 세로는 90\\~240사이에 데이터가 많이 모여 있는 것을 볼 수 있다. 그래서 중간값이라고 생각되는 크기를 사용해서 가로는 140, 세로는 180을 사용하겠다. "]},{"cell_type":"markdown","metadata":{"id":"AtbHt4gbpcqH"},"source":["# 3. 데이터 전 처리\n","---\n","\n","\n","## 3.1 통일된 이미지 크기로 이미지 가져오기\n","\n","이제 통일된 이미지 크기로 이미지를 가져오겠다. 이미지는 위에 결정한대로 가로 140, 세로 180으로 크기를 통일시킨다. 그리고 이미지를 가져올 때 마스크를 쓴 사진의 라벨은 1로 하고, 마스크를 쓰지 않은 사진의 라벨은 0으로 구분한다. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QetWj-NBpcqH"},"outputs":[],"source":["image_height = 180\n","image_width = 140\n","\n","images = []\n","labels = []\n","\n","for non_mask in non_mask_list:\n","    image = load_img('c:\\\\non_mask\\\\'+non_mask, target_size=(image_height, image_width))\n","    image = img_to_array(image)\n","    images.append(image)\n","    labels.append(0)\n","    \n","for mask in mask_list:\n","    image = load_img('c:\\\\mask\\\\'+mask, target_size=(image_height, image_width))\n","    image = img_to_array(image)\n","    images.append(image)\n","    labels.append(1)"]},{"cell_type":"markdown","metadata":{"id":"LI_9S2yspcqH"},"source":["images[0]의 shape를 출력하니 세로 180, 가로 140, 채널 3인 것을 확인 할 수 있다. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V4rSfMLPpcqH"},"outputs":[],"source":["images[0].shape"]},{"cell_type":"markdown","metadata":{"id":"na14ldKnpcqI"},"source":["## 3.2 데이터 분리하기\n","\n","가져온 이미지를 sklearn.model_selection에서 train_test_split()를 사용하여 훈련용 데이터와 검증용 데이터, 테스트용 데이터를 8:1:1 비율로 나눈다. 그리고 분리시 마스크를 쓴 사진과 마스크를 쓰지 않은 사진이 섞이도록 한다. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ewOryMCpcqI"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","import numpy as np\n","\n","x_train, x_test, y_train, y_test = train_test_split(np.array(images), np.array(labels), test_size=0.1)\n","x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1)"]},{"cell_type":"markdown","metadata":{"id":"Kntst-uhpcqI"},"source":["y_train을 출력해보니 사진이 잘 섞인 것을 확인 할 수 있다. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VSNcd4kbpcqI"},"outputs":[],"source":["y_train"]},{"cell_type":"markdown","metadata":{"id":"RgswzoyTpcqI"},"source":["# 4. 딥러닝 모델 만들고 학습하기\n","\n","---\n","\n","## 4.1 CNN 모델 설정\n","\n","이제 딥러닝에 사용할 CNN 모델을 구현한다. 모델의 층 구성은 AI 온라인 교육 내용을 참고하여 만들었다. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wbFwkB4npcqJ"},"outputs":[],"source":["model = Sequential([    \n","    Conv2D(filters=32, kernel_size=(3,3), activation='relu', input_shape=(image_height, image_width, image_channel)),\n","    MaxPool2D(pool_size=(2, 2)),\n","    Dropout(rate=0.25),\n","    \n","    Conv2D(filters=64, kernel_size=(3,3), activation='relu'),\n","    MaxPool2D(pool_size=(2, 2)),\n","    Dropout(rate=0.25),\n","    \n","    Flatten(),\n","    Dense(512, activation='relu'),\n","    Dropout(rate=0.25),\n","    Dense(2, activation='softmax')\n","])\n","\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"6JSfXsyLpcqJ"},"source":["## 4.2 모델 컴파일하기\n","\n","모델의 출력은 마스크를 썼는지 안 썼는지를 나타내야 하기 때문에 loss 함수는 sparse_categorical_crossentropy로 설정하고 optimizer는 adam을 사용하게 하겠다. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n2krIzK6pcqJ"},"outputs":[],"source":["model.compile(\n","    loss='sparse_categorical_crossentropy',\n","    optimizer='adam',\n","    metrics=['accuracy']\n",")"]},{"cell_type":"markdown","metadata":{"id":"AbzaVOywpcqJ"},"source":["## 4.3 모델 학습시키기\n","\n","이렇게 만들진 모델에 학습 데이터와 검증 데이터를 사용하여 학습시킨다. "]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"V9iwCD1ypcqK"},"outputs":[],"source":["EPOCHS = 30\n","\n","history = model.fit(x_train, \n","                    y_train,\n","                    validation_data = (x_val, y_val),\n","                    epochs=EPOCHS, \n","                   )"]},{"cell_type":"markdown","metadata":{"id":"PichpR2TpcqK"},"source":["모델을 학습하면서 출력되는 손실값과 정확도를 확인할 수 있다. 15에포크부터는 검증 데이터의 정확도가 100%인 것을 확인 할 수 있다. "]},{"cell_type":"markdown","metadata":{"id":"L6hwTWENpcqK"},"source":["## 4.4 학습을 수행시 Accuracy와 Loss 변화 그래프로 확인하기\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"Wfe4VhYYpcqK"},"outputs":[],"source":["accuracy = history.history['accuracy']\n","val_accuracy = history.history['val_accuracy']\n","\n","loss=history.history['loss']\n","val_loss=history.history['val_loss']\n","\n","epochs_range = range(EPOCHS)\n","\n","plt.figure(figsize=(16, 8))\n","plt.subplot(1, 2, 1)\n","plt.plot(epochs_range, accuracy, label='Training Accuracy')\n","plt.plot(epochs_range, val_accuracy, label='Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.title('Training and Validation Accuracy')\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(epochs_range, loss, label='Training Loss')\n","plt.plot(epochs_range, val_loss, label='Validation Loss')\n","plt.legend(loc='upper right')\n","plt.title('Training and Validation Loss')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"ePe38xDwpcqK"},"source":["## 4.5 모델 성능 평가 및 예측\n","\n","학습이 잘 되었는지 평가용 데이터로 평가를 해보자. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F6ZE6UEjpcqL"},"outputs":[],"source":["test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n","\n","print('test set accuracy: ', test_accuracy)"]},{"cell_type":"markdown","metadata":{"id":"S5YoBB8gpcqL"},"source":["정확도가 100%로 나옵니다. \n","실제 이미지를 출력해서 실제 라벨과 예측값을 출력해보자. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BB55Ie6zpcqL"},"outputs":[],"source":["test_prediction = np.argmax(model.predict(x_test), axis=-1)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"Q8Cl6jYypcqL"},"outputs":[],"source":["plt.figure(figsize = (13, 13))\n","\n","start_index = 0\n","for i in range(25):\n","    plt.subplot(5, 5, i + 1)\n","    plt.grid(False)\n","    plt.xticks([])\n","    plt.yticks([])\n","    prediction = test_prediction[start_index + i]\n","    actual = y_test[start_index + i]\n","    col = 'g'\n","    if prediction != actual:\n","        col = 'r'\n","    plt.xlabel('Actual={} || Pred={}'.format(actual, prediction), color = col)\n","    plt.imshow(array_to_img(x_test[start_index + i]))\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"6bFJrsAcpcqL"},"source":["마지막으로 confusion matrix를 시각화하여 분류 학습 결과를 확인해 보자. 29개의 마스크를 쓰지 않은 이미지와 31개의 마스크를 쓴 이미지를 정확하게 예측한 것을 확인할 수 있다. "]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"qibU_NI7pcqM"},"outputs":[],"source":["cm = confusion_matrix(y_test, test_prediction)\n","sns.heatmap(cm, annot = True)"]},{"cell_type":"markdown","metadata":{"id":"v2dpsN-lpcqM"},"source":["# 5. 웹캠을 사용하여 실시간으로 마스크 착용 유무 확인하기\n","\n","---\n","\n","이제 웹캠을 사용하여 실시간으로 마스크 착용 유무를 확인하려고 한다. 실시간으로 확인을 하기 위해서 opencv를 사용할 것이다.\n","\n","\n","## 5.1 모델 저장하기 \n","\n","지금까지 학습한 모델을 저장할 수 있다. model에 save() 메서드를 사용하면 모델을 저장할 수 있다. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6CS1ACPjpcqM"},"outputs":[],"source":["model.save('detect_face_model.h5')"]},{"cell_type":"markdown","metadata":{"id":"HGUsGIQLpcqM"},"source":["## 5.2 모델 불러오기\n","\n","저장한 모델은 load_model() 함수를 사용하여 불러올 수 있다. 이렇게 불러온 모델은 이미 학습이 되어 있는 상태이므로 바로 입력 데이터를 사용해서 예측이 가능하다. 그리고 불러온 모델은 summary()를 통해서 모델이 어떻게 구성되어 있는지 확인할 수 있다. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mfbD-qOHpcqM"},"outputs":[],"source":["model = load_model('detect_face_model.h5')\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"_CxWkq1ApcqN"},"source":["## 5.3. 웹캠으로 캡쳐한 얼굴 예측해보기\n","\n","웹캠으로 캡쳐한 얼굴을 예측하기 위해서는 학습을 하기 위해서 사용했던 데이터와 동일한 방법으로 전처리를 해야 한다. 대부분의 코드는 데이터 수집에서 사용한 코드와 동일하다. 하지만 학습에 사용한 데이터들을 여러개의 이미지를 입력하였지만, 실시간으로 캡쳐한 데이터는 1개라는 점이 다르다. 그래서 캡쳐한 데이터는 1개의 데이터를 가지는 배열로 변환시켜야 한다. \n","\n","배열로 변화시키기 위해서 np.expand_dims()를 사용한다. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fEGWanfppcqN"},"outputs":[],"source":["faces, confidences = cvlib.detect_face(frame)   # 얼굴 탐지하기\n","start_x, start_y, end_x, end_y = face[0]\n","\n","face_frame = frame[start_y:end_y, start_x:end_x, :]   # 얼굴 영역\n","\n","cv2.imwrite('temp.jpg', face_frame)\n","x = load_img('temp.jpg', target_size=(image_height, image_width))\n","print(\"origin_x.shape: \", x.shape)\n","\n","x = img_to_array(x)\n","x = np.expand_dims(x, axis=0)\n","print(\"expand_dims_x.shape: \", x.shape)\n","\n","pre = model.predict(x)\n","is_wear_mask = np.argmax(pre)\n","print(\"model.predict(): \", pre)\n","print(\"is_wear_mask: \", is_wear_mask\")"]},{"cell_type":"markdown","metadata":{"id":"TaBkuNdapcqN"},"source":["## 5.4 최종 프로그램 작성\n","\n","위의 내용을 사용해서 웹캠을 사용하여 실시간으로 마스크 착용 유무를 확인하는 프로그램을 만들어 보자. 이 코드는 예측한 결과를 이미지에 얼굴 영역을 네모 박스로 그리고 마스크 착용 여부를 출력하기 위해서 cv2.imshow()를 사용한다. cv2.imshow()는 주피터 노트북에서 제대로 작동하지 않기 때문에 파이썬 코드로 따로 실행을 시켜야 한다. 이 프로그램은 q 키보드를 누르기 전까지 계속 실행되게 만들었다. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DnF0rXpcpcqN"},"outputs":[],"source":["Image(\"mask_detect_program.png\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"luPJU5CapcqN"},"outputs":[],"source":["import cvlib\n","import cv2\n","import numpy as np\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.applications.resnet50 import preprocess_input\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","from PIL import ImageFont, ImageDraw, Image\n","\n","image_height = 180\n","image_width = 140\n","\n","model = load_model('detect_face_model.h5')  # 학습한 모델 불러오기\n","\n","# 웹캠 연결하기\n","webcam = cv2.VideoCapture(0)\n","if not webcam.isOpened():\n","    raise Exception(\"웹캡이 연결되지 않았습니다.\")\n","\n","while webcam.isOpened():\n","    is_readed, frame = webcam.read()  # 웹캠으로 촬영하기\n","\n","    if not is_readed:\n","        raise Exception(\"캡쳐가 제대로 되지 않았습니다.\")\n","\n","    faces, confidences = cvlib.detect_face(frame)   # 얼굴 탐지하기\n","\n","    for face, confidence in zip(faces, confidences):\n","        if confidence < 0.8:   # 얼굴일 확률이 80% 이하면 패스한다.\n","            continue\n","        start_x, start_y, end_x, end_y = face\n","\n","        try:\n","            face_frame = frame[start_y:end_y, start_x:end_x, :]   # 얼굴 영역 추출하기\n","\n","            cv2.imwrite('temp.jpg', face_frame)\n","            x = load_img('temp.jpg', target_size=(image_height, image_width))\n","\n","            x = img_to_array(x) \n","            x = np.expand_dims(x, axis=0)   # 예측을 위해 배열로 만들기\n","\n","            pre = model.predict(x)\n","\n","            is_wear_mask = np.argmax(pre)   # 마스크 착용 여부 결정 - 1: 마스크 착용, 0: 마스크 미착용\n","            \n","            # 마스크 착용 미착용에 따라 색상 및 문구를 설정\n","            print_text = 'No mask'\n","            color = (0, 0, 255)\n","            if is_wear_mask:\n","                print_text = 'Mask'\n","                color = (0, 255, 0)\n","                \n","            # 캡처한 이미지에 얼굴 영역을 네모 박스와 마스크 착용 문구를 출력한다.\n","            cv2.rectangle(frame, (start_x, start_y),\n","                          (end_x, end_y), color, 2)\n","            y = start_y - 10 if start_y - 10 > 10 else start_y + 10\n","            text = print_text\n","            cv2.putText(frame, text, (start_x, y),\n","                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n","        except:\n","            break\n","    \n","    cv2.imshow(\"Mask detect\", frame)   # 마스크 착용 여부를 볼 수 있는 창을 연다. \n","\n","    if cv2.waitKey(1) & 0xFF == ord('q'):   # q를 입력시 프로그램이 종료된다. \n","        break\n","\n","cv2.destroyAllWindows()\n","webcam.release()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"Detect_mask.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}